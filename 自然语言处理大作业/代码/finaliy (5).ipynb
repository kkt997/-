{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14270261,"sourceType":"datasetVersion","datasetId":9106759}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# å®‰è£…å¿…è¦çš„åº“\n!pip install -U openai scikit-learn tqdm pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:21.549552Z","iopub.execute_input":"2025-12-27T08:53:21.549908Z","iopub.status.idle":"2025-12-27T08:53:43.838545Z","shell.execute_reply.started":"2025-12-27T08:53:21.549870Z","shell.execute_reply":"2025-12-27T08:53:43.837471Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\nCollecting openai\n  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nCollecting scikit-learn\n  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nCollecting pandas\n  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\nRequirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDownloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, pandas, openai\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.6.1\n    Uninstalling scikit-learn-1.6.1:\n      Successfully uninstalled scikit-learn-1.6.1\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\n  Attempting uninstall: openai\n    Found existing installation: openai 1.109.1\n    Uninstalling openai-1.109.1:\n      Successfully uninstalled openai-1.109.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ndask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed openai-2.14.0 pandas-2.3.3 scikit-learn-1.8.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport time\nimport json\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom openai import OpenAI\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset\n# ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ notebook ä¸“ç”¨è¿›åº¦æ¡ï¼Œæ˜¾ç¤ºæ›´ç¨³å®š\nfrom tqdm.notebook import tqdm \nfrom sklearn.metrics import accuracy_score, f1_score\n\nuser_secrets = UserSecretsClient()\nSILICONFLOW_API_KEY = user_secrets.get_secret(\"SILICONFLOW_API_KEY\")\n\nclient = OpenAI(\n    api_key=SILICONFLOW_API_KEY,\n    base_url=\"https://api.siliconflow.cn/v1\"\n)\n\nMODEL_NAME = \"deepseek-ai/DeepSeek-V3.2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:43.839845Z","iopub.execute_input":"2025-12-27T08:53:43.840230Z","iopub.status.idle":"2025-12-27T08:53:53.393937Z","shell.execute_reply.started":"2025-12-27T08:53:43.840173Z","shell.execute_reply":"2025-12-27T08:53:53.392869Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# --- 1. æ ¸å¿ƒè·¯å¾„é…ç½® ---\nDATA_ROOT = \"/kaggle/input/text-cls/data\"\n\n# --- 2. å¢å¼ºç‰ˆæ•°æ®æ˜ å°„ (æŒ‡å®šç¡®åˆ‡æ–‡ä»¶å) ---\nDATA_MAPPINGS = {\n    \"us_economy\": {\n        \"folder\": \"01-nyt-sentiment\",\n        # æˆªå›¾æ˜¾ç¤ºç¡®åˆ‡æ–‡ä»¶åä¸º 3SU.csv\n        \"filename\": \"3SU.csv\",\n        \"text_col\": \"text\",   # ç¨åä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œå¦‚æœä¸å« text ä¼šå°è¯•æ™ºèƒ½åŒ¹é…\n        \"label_col\": \"label\", # 3SU æ•°æ®é›†é€šå¸¸å« label æˆ– sentiment\n        \"id2label\": {0: \"Negative Sentiment\", 1: \"Positive Sentiment\"}\n    },\n    \"kavanaugh\": {\n        \"folder\": \"02-twitter-stance\",\n        # æˆªå›¾æ˜¾ç¤ºç¡®åˆ‡æ–‡ä»¶åä¸º kavanaugh_tweets_groundtruth.csv\n        \"filename\": \"kavanaugh_tweets_groundtruth.csv\",\n        \"text_col\": \"text\",   # è®ºæ–‡æåˆ°åˆ—åæ˜¯ text\n        \"label_col\": \"stance\", # è®ºæ–‡æåˆ°åˆ—åæ˜¯ stance\n        \"id2label\": {0: \"negative attitudinal stance towards\", 1: \"positive attitudinal stance towards\"}\n    },\n    # é’ˆå¯¹ 03 å’Œ 04 æ–‡ä»¶å¤¹ï¼Œæˆªå›¾ä¸­æ˜¾ç¤ºåªæœ‰ all-x å’Œ all-yï¼Œéœ€è¦åˆå¹¶\n    \"german_anger\": {\n        \"folder\": \"03-emotion-angry\",\n        \"filename\": \"all-x-translated.csv\", # ä¼˜å…ˆè¯»å–ç¿»è¯‘åçš„æ–‡æœ¬\n        \"label_filename\": \"all-y.csv\",      # æ ‡ç­¾åœ¨å¦ä¸€ä¸ªæ–‡ä»¶\n        \"text_col\": \"text\",\n        \"label_col\": \"anger_label\",\n        \"id2label\": {0: \"Non-Angry\", 1: \"Angry\"}\n    },\n    \"eu_position\": {\n        \"folder\": \"04-brexit-stance\",\n        \"filename\": \"all-x.csv\",       # æ–‡æœ¬\n        \"label_filename\": \"all-y.csv\", # æ ‡ç­¾\n        \"text_col\": \"text\",\n        \"label_col\": \"position\",\n        \"id2label\": {0: \"Neutral towards Leave demands\", 1: \"Pro-Leave demands\", 2: \"Very Pro-Leave demands\"}\n    }\n}\n\n# --- 2. åŸè®ºæ–‡ä¸­çš„ Prompt ---\nPROMPTS_ORIGINAL = {\n    \"us_economy\": {\n        \"instruction\": \"You have been assigned the task of zero-shot text classification for sentiment analysis. Your objective is to classify a given text snippet into one of several possible class labels, based on the sentiment expressed in the text. Your output should consist of a single class label that best matches the sentiment expressed in the text. Your output should consist of a single class label that best matches the given text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.\",\n        \"labels\": [\"Negative Sentiment\", \"Positive Sentiment\"]\n    },\n    \"kavanaugh\": {\n        \"instruction\": \"You have been assigned the task of zero-shot text classification for stance classification. Your objective is to classify a given text snippet into one of several possible class labels, based on the attitudinal stance towards the given text. Your output should consist of a single class label that best matches the stance expressed in the text. Your output should consist of a single class label that best matches the given text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.\",\n        \"labels\": [\"negative attitudinal stance towards\", \"positive attitudinal stance towards\"]\n    },\n    \"german_anger\": {\n        \"instruction\": \"You have been assigned the task of zero-shot text classification for emotion classification. Your objective is to classify a given text snippet into one of several possible class labels, based on the anger level in the given text. Your output should consist of a single class label that best matches the anger expressed in the text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.\",\n        \"labels\": [\"Angry\", \"Non-Angry\"]\n    },\n    \"eu_position\": {\n        \"instruction\": \"You have been assigned the task of zero-shot text classification for political texts on attitudinal stance towards Brexit and leave demands related to the European Union (EU). Your objective is to classify a given text snippet into one of several possible class labels, based on the stance towards Brexit and general leave demands in the given text. Your output should consist of a single class label that best matches the content expressed in the text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.\",\n        \"labels\": [\"Neutral towards Leave demands\", \"Pro-Leave demands\", \"Very Pro-Leave demands\"]\n    }\n}\n\n# --- 3. ä¼˜åŒ–åçš„ Prompt ---\nPROMPTS_OPTIMIZED = {\n    \"us_economy\": {\n        \"instruction\": \"\"\"### Role\nYou are an expert financial analyst.\n\n### Task\nClassify the sentiment of the following news snippet regarding the US Economy.\n\n### Constraints\n- Output exactly ONE label from the list below.\n- Do NOT output explanations or punctuation.\"\"\",\n        \"labels\": [\"Negative Sentiment\", \"Positive Sentiment\"]\n    },\n    \"kavanaugh\": {\n        \"instruction\": \"\"\"### Role\nYou are a political stance detector specializing in social media text.\n\n### Task\nDetermine if the author of the tweet supports or opposes the nomination of Brett Kavanaugh.\n\n### Constraints\n- Output exactly ONE label from the list below.\n- Do NOT output explanations.\"\"\",\n        \"labels\": [\"negative attitudinal stance towards\", \"positive attitudinal stance towards\"]\n    },\n    \"german_anger\": {\n        \"instruction\": \"\"\"### Role\nYou are a linguistic expert specializing in emotion detection in political discourse.\n\n### Task\nAnalyze the text and determine if it expresses 'Anger'.\n\n### Constraints\n- Output exactly ONE label from the list below.\n- Do NOT output explanations.\"\"\",\n        \"labels\": [\"Angry\", \"Non-Angry\"]\n    },\n    \"eu_position\": {\n        \"instruction\": \"\"\"### Role\nYou are a political scientist specializing in EU affairs and Brexit.\n\n### Task\nClassify the party's position regarding Brexit and EU leave demands.\n\n### Constraints\n- Output exactly ONE label from the list below.\n- Do NOT output explanations.\"\"\",\n        \"labels\": [\"Neutral towards Leave demands\", \"Pro-Leave demands\", \"Very Pro-Leave demands\"]\n    }\n}\n\n# --- 4. æ™ºèƒ½æ•°æ®åŠ è½½å‡½æ•° (æ ¸å¿ƒä¿®å¤) ---\ndef load_dataset_smart(task_key):\n    config = DATA_MAPPINGS[task_key]\n    folder_path = os.path.join(DATA_ROOT, config['folder'])\n    \n    # æƒ…å†µ A: å­˜åœ¨ label_filenameï¼Œè¯´æ˜æ˜¯åˆ†ç¦»çš„æ•°æ® (03, 04 æ–‡ä»¶å¤¹)\n    if \"label_filename\" in config:\n        print(f\"ğŸ“‚ Detected split files for {task_key}. Merging...\")\n        try:\n            # è¯»å–æ–‡æœ¬ (all-x)\n            path_x = os.path.join(folder_path, config['filename'])\n            df_x = pd.read_csv(path_x, encoding='latin1')\n            # å¦‚æœæ²¡æœ‰è¡¨å¤´ï¼Œå¼ºåˆ¶è®¾ä¸º text\n            if config['text_col'] not in df_x.columns:\n                df_x = pd.read_csv(path_x, encoding='latin1', header=None, names=[config['text_col']])\n            \n            # è¯»å–æ ‡ç­¾ (all-y)\n            path_y = os.path.join(folder_path, config['label_filename'])\n            df_y = pd.read_csv(path_y, encoding='latin1')\n            # å¦‚æœæ²¡æœ‰è¡¨å¤´ï¼Œå¼ºåˆ¶è®¾ä¸º label_col\n            if config['label_col'] not in df_y.columns:\n                 df_y = pd.read_csv(path_y, encoding='latin1', header=None, names=[config['label_col']])\n            \n            # åˆå¹¶\n            df = pd.concat([df_x, df_y], axis=1)\n            return df\n        except Exception as e:\n            print(f\"âŒ Merge failed: {e}\")\n            return None\n\n    # æƒ…å†µ B: å•ä¸ªæ–‡ä»¶ (01, 02 æ–‡ä»¶å¤¹)\n    else:\n        file_path = os.path.join(folder_path, config['filename'])\n        print(f\"ğŸ“„ Loading specific file: {config['filename']}\")\n        try:\n            # å°è¯•ä¸åŒçš„ç¼–ç \n            try:\n                df = pd.read_csv(file_path)\n            except:\n                df = pd.read_csv(file_path, encoding='latin1')\n            \n            # è‡ªåŠ¨ä¿®æ­£åˆ—å (å¦‚æœ csv é‡Œå« 'tweet_text' ä½†é…ç½®å« 'text')\n            # ç®€å•çš„åˆ—åè§„èŒƒåŒ–\n            df.columns = [c.lower() for c in df.columns]\n            \n            # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°è¯•æ¨¡ç³ŠåŒ¹é…\n            if config['text_col'] not in df.columns:\n                # æ‰¾ä¸€ä¸ªçœ‹èµ·æ¥åƒæ–‡æœ¬çš„åˆ—\n                candidates = ['sentence', 'tweet_text', 'tweet', 'content', 'text']\n                for cand in candidates:\n                    if cand in df.columns:\n                        print(f\"âš ï¸ Renaming column '{cand}' to '{config['text_col']}'\")\n                        df = df.rename(columns={cand: config['text_col']})\n                        break\n            \n            return df\n            \n        except FileNotFoundError:\n            print(f\"âŒ File not found: {file_path}\")\n            return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:53.396068Z","iopub.execute_input":"2025-12-27T08:53:53.396612Z","iopub.status.idle":"2025-12-27T08:53:53.415550Z","shell.execute_reply.started":"2025-12-27T08:53:53.396575Z","shell.execute_reply":"2025-12-27T08:53:53.414483Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef run_comparison(task_key, prompt_type=\"original\", sample_size=None):\n    # 1. åŠ è½½æ•°æ®\n    df = load_dataset_smart(task_key)\n    \n    if df is None:\n        print(f\"âŒ Failed to load data for {task_key}\")\n        return None\n        \n    data_config = DATA_MAPPINGS[task_key]\n    \n    # æ£€æŸ¥æ–‡æœ¬åˆ—\n    if data_config['text_col'] not in df.columns:\n        print(f\"âŒ Column '{data_config['text_col']}' missing. Found: {df.columns.tolist()}\")\n        return None\n        \n    # è‡ªåŠ¨æ£€æŸ¥æ ‡ç­¾åˆ—\n    label_col = data_config['label_col']\n    if label_col not in df.columns:\n        print(f\"âŒ Label column '{label_col}' not found. Found: {df.columns.tolist()}\")\n        return None\n\n    if sample_size:\n        df = df.head(sample_size)\n\n    # 2. é€‰æ‹© Prompt\n    if prompt_type == \"original\":\n        prompt_config = PROMPTS_ORIGINAL[task_key] \n    else:\n        prompt_config = PROMPTS_OPTIMIZED[task_key]\n\n    print(f\"ğŸš€ Running {task_key} | Mode: {prompt_type.upper()} | Samples: {len(df)}\")\n    \n    results_records = []\n    true_labels = []\n    pred_labels = []\n    \n    pbar = tqdm(df.iterrows(), total=len(df), desc=\"Processing\", ncols=800)\n    \n    for idx, row in pbar:\n        text = str(row[data_config['text_col']])\n        \n        # --- ã€å…³é”®ä¿®å¤ã€‘é€‚é…å­—ç¬¦ä¸²æ ‡ç­¾ ---\n        try:\n            raw_label = row[label_col]\n            \n            # å¦‚æœæ˜¯ NaN (ç©ºå€¼)ï¼Œè·³è¿‡\n            if pd.isna(raw_label):\n                continue\n\n            # è½¬æˆå­—ç¬¦ä¸²å¹¶å°å†™ï¼Œå»åŒ¹é…é…ç½®é‡Œçš„é”® (ä¾‹å¦‚ \"positive\")\n            # è¿™æ · 'mixed' å› ä¸ºä¸åœ¨ id2label é‡Œï¼Œå°±ä¼šè¢«è‡ªåŠ¨è·³è¿‡ï¼Œç¬¦åˆè®ºæ–‡è¿‡æ»¤é€»è¾‘\n            label_key = str(raw_label).lower().strip()\n            \n            if label_key in data_config['id2label']:\n                true_text = data_config['id2label'][label_key]\n            else:\n                # å°è¯•çœ‹çœ‹æ˜¯ä¸æ˜¯æ•°å­— (å…¼å®¹å…¶ä»–æ•°æ®é›†)\n                try:\n                    int_label = int(raw_label)\n                    if int_label in data_config['id2label']:\n                        true_text = data_config['id2label'][int_label]\n                    else:\n                        continue # æ—¢ä¸æ˜¯å·²çŸ¥å­—ç¬¦ä¸²ï¼Œä¹Ÿä¸æ˜¯å·²çŸ¥æ•°å­—ï¼Œè·³è¿‡\n                except:\n                    continue # æ— æ³•è½¬æ¢ï¼Œè·³è¿‡\n                    \n        except Exception as e:\n            continue \n            \n        # æ„å»º Prompt\n        labels_str = \", \".join([f\"'{l}'\" for l in prompt_config['labels']])\n        \n        if prompt_type == \"original\":\n            final_prompt = f\"{prompt_config['instruction']}\\n\\nText: {text}\\nLabels: {labels_str}\\nAnswer:\"\n        else:\n            final_prompt = f\"{prompt_config['instruction']}\\n\\n### Labels\\n{labels_str}\\n\\n### Input Text\\n{text}\\n\\n### Answer\"\n\n        # è°ƒç”¨ LLM\n        raw_pred = query_llm(final_prompt)\n        pred_clean = normalize_prediction(raw_pred, prompt_config['labels'])\n        \n        # è®°å½•\n        results_records.append({\n            \"text\": text,\n            \"true_label\": true_text,\n            \"pred_label\": pred_clean,\n            \"is_correct\": true_text == pred_clean,\n            \"raw_response\": raw_pred \n        })\n        \n        true_labels.append(true_text)\n        pred_labels.append(pred_clean)\n        \n        pbar.set_postfix({\"Last\": pred_clean[:10]})\n        time.sleep(0.3)\n\n    if not true_labels:\n        print(\"âš ï¸ No valid labels found. Please check your data mappings.\")\n        return None\n\n    # è®¡ç®—æŒ‡æ ‡\n    print(f\"\\nğŸ“Š Detailed Classification Report ({prompt_type.upper()}):\")\n    print(classification_report(true_labels, pred_labels, zero_division=0))\n\n    result_filename = f\"prediction_{task_key}_{prompt_type}.csv\"\n    pd.DataFrame(results_records).to_csv(result_filename, index=False, encoding='utf-8-sig')\n    print(f\"ğŸ’¾ Saved to: {result_filename}\")\n\n    acc = accuracy_score(true_labels, pred_labels)\n    f1 = f1_score(true_labels, pred_labels, average='macro')\n    \n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:53.416907Z","iopub.execute_input":"2025-12-27T08:53:53.417325Z","iopub.status.idle":"2025-12-27T08:53:53.456517Z","shell.execute_reply.started":"2025-12-27T08:53:53.417280Z","shell.execute_reply":"2025-12-27T08:53:53.455491Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import time\n\ndef find_csv_in_folder(folder_path):\n    \"\"\"åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶\"\"\"\n    if not os.path.exists(folder_path):\n        raise FileNotFoundError(f\"Directory not found: {folder_path}\")\n    \n    for file in os.listdir(folder_path):\n        if file.endswith(\".csv\"):\n            return os.path.join(folder_path, file)\n    raise FileNotFoundError(f\"No CSV file found in {folder_path}\")\n\ndef query_llm(prompt):\n    \"\"\"\n    è°ƒç”¨ LLMï¼ŒåŒ…å«è‡ªåŠ¨é‡è¯• (é™é€Ÿ/ç³»ç»Ÿç¹å¿™) å’Œ è‡ªåŠ¨è·³è¿‡ (æ•æ„Ÿå†…å®¹)\n    \"\"\"\n    # æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨\n    if not SILICONFLOW_API_KEY or \"sk-\" not in SILICONFLOW_API_KEY:\n        tqdm.write(\"âŒ Critical: API Key is missing or invalid!\")\n        return None\n\n    max_retries = 10 # æœ€å¤§é‡è¯•æ¬¡æ•°\n    base_wait_time = 3 # åŸºç¡€ç­‰å¾…æ—¶é—´\n\n    for attempt in range(max_retries):\n        try:\n            response = client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1, \n            )\n            return response.choices[0].message.content.strip()\n            \n        except Exception as e:\n            error_msg = str(e).lower()\n            \n            # --- æƒ…å†µ 1: æ•æ„Ÿå†…å®¹æ‹¦æˆª (400) ---\n            if \"400\" in error_msg and (\"input data\" in error_msg or \"sensitive\" in error_msg):\n                # tqdm.write(f\"âš ï¸ è·³è¿‡æ•æ„Ÿå†…å®¹\") # å¯é€‰ï¼šæ‰“å°æ—¥å¿—\n                return None \n\n            # --- æƒ…å†µ 2: é™é€Ÿ (429) æˆ– æœåŠ¡ç«¯ç¹å¿™ (500/502/503/busy) ---\n            elif (\"rpm\" in error_msg or \"429\" in error_msg or \"limit\" in error_msg or \n                  \"503\" in error_msg or \"502\" in error_msg or \"500\" in error_msg or \"busy\" in error_msg):\n                \n                wait_time = base_wait_time * (attempt + 1)\n                # æ‰“å°æç¤º\n                if \"503\" in error_msg or \"busy\" in error_msg:\n                    tqdm.write(f\"â³ æœåŠ¡å™¨ç¹å¿™ (503 Busy)ï¼Œç­‰å¾… {wait_time}s åé‡è¯•... ({attempt+1}/{max_retries})\")\n                else:\n                    tqdm.write(f\"â³ é™é€Ÿä¸­ï¼Œç­‰å¾… {wait_time}s åé‡è¯•...\")\n                \n                time.sleep(wait_time)\n                \n            # --- æƒ…å†µ 3: å…¶ä»–é”™è¯¯ (å¦‚è®¤è¯å¤±è´¥) ---\n            else:\n                tqdm.write(f\"âš ï¸ API Error: {error_msg[:100]}...\")\n                return None\n    \n    tqdm.write(\"âŒ é‡è¯•å¤šæ¬¡å¤±è´¥ï¼Œæ”¾å¼ƒè¯¥æ ·æœ¬ã€‚\")\n    return None\n\ndef normalize_prediction(pred_raw, valid_labels):\n    if not pred_raw:\n        return \"Unknown\"\n    \n    pred_lower = pred_raw.lower().strip()\n    pred_lower = pred_lower.rstrip(\".,\\\"'\")\n    \n    for label in valid_labels:\n        if label.lower() in pred_lower:\n            return label\n            \n    return \"Unknown\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:53.457692Z","iopub.execute_input":"2025-12-27T08:53:53.458035Z","iopub.status.idle":"2025-12-27T08:53:53.472655Z","shell.execute_reply.started":"2025-12-27T08:53:53.457996Z","shell.execute_reply":"2025-12-27T08:53:53.471518Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, f1_score\n\ndef run_comparison(task_key, prompt_type=\"original\", sample_size=None, start_idx=0):\n    # 1. åŠ è½½æ•°æ®\n    df = load_dataset_smart(task_key)\n    \n    if df is None:\n        print(f\"âŒ Failed to load data for {task_key}\")\n        return None\n        \n    data_config = DATA_MAPPINGS[task_key]\n    \n    # æ£€æŸ¥æ–‡æœ¬åˆ—\n    if data_config['text_col'] not in df.columns:\n        print(f\"âŒ Column '{data_config['text_col']}' missing. Found: {df.columns.tolist()}\")\n        return None\n        \n    # è‡ªåŠ¨æ£€æŸ¥æ ‡ç­¾åˆ—\n    label_col = data_config['label_col']\n    if label_col not in df.columns:\n        print(f\"âŒ Label column '{label_col}' not found. Found: {df.columns.tolist()}\")\n        return None\n\n    if sample_size:\n        df = df.head(sample_size)\n\n    total_samples = len(df)\n\n    # 2. é€‰æ‹© Prompt\n    if prompt_type == \"original\":\n        prompt_config = PROMPTS_ORIGINAL[task_key] \n    else:\n        prompt_config = PROMPTS_OPTIMIZED[task_key]\n\n    print(\n        f\"ğŸš€ Running {task_key} | Mode: {prompt_type.upper()} | \"\n        f\"Samples: {total_samples} | Start from index: {start_idx}\"\n    )\n    \n    results_records = []\n    true_labels = []\n    pred_labels = []\n    \n    pbar = tqdm(\n        df.iterrows(),\n        total=total_samples,\n        desc=\"Processing\",\n        ncols=800,\n        initial=start_idx\n    )\n    \n    for idx, row in pbar:\n        # âœ… æ–­ç‚¹ç»­è·‘ï¼šè·³è¿‡å·²å®Œæˆæ ·æœ¬\n        if idx < start_idx:\n            continue\n\n        text = str(row[data_config['text_col']])\n        \n        # --- é€‚é…å­—ç¬¦ä¸² / æ•°å­—æ ‡ç­¾ ---\n        try:\n            raw_label = row[label_col]\n            \n            if pd.isna(raw_label):\n                continue\n\n            label_key = str(raw_label).lower().strip()\n            \n            if label_key in data_config['id2label']:\n                true_text = data_config['id2label'][label_key]\n            else:\n                try:\n                    int_label = int(raw_label)\n                    if int_label in data_config['id2label']:\n                        true_text = data_config['id2label'][int_label]\n                    else:\n                        continue\n                except:\n                    continue\n                    \n        except Exception:\n            continue \n            \n        # æ„å»º Prompt\n        labels_str = \", \".join([f\"'{l}'\" for l in prompt_config['labels']])\n        \n        if prompt_type == \"original\":\n            final_prompt = (\n                f\"{prompt_config['instruction']}\\n\\n\"\n                f\"Text: {text}\\n\"\n                f\"Labels: {labels_str}\\n\"\n                f\"Answer:\"\n            )\n        else:\n            final_prompt = (\n                f\"{prompt_config['instruction']}\\n\\n\"\n                f\"### Labels\\n{labels_str}\\n\\n\"\n                f\"### Input Text\\n{text}\\n\\n\"\n                f\"### Answer\"\n            )\n\n        # è°ƒç”¨ LLM\n        raw_pred = query_llm(final_prompt)\n        pred_clean = normalize_prediction(raw_pred, prompt_config['labels'])\n        \n        # è®°å½•\n        results_records.append({\n            \"index\": idx,\n            \"text\": text,\n            \"true_label\": true_text,\n            \"pred_label\": pred_clean,\n            \"is_correct\": true_text == pred_clean,\n            \"raw_response\": raw_pred \n        })\n        \n        true_labels.append(true_text)\n        pred_labels.append(pred_clean)\n        \n        pbar.set_postfix({\"Last\": pred_clean[:10]})\n        time.sleep(0.3)\n\n    if not true_labels:\n        print(\"âš ï¸ No valid labels found. Please check your data mappings.\")\n        return None\n\n    # è®¡ç®—æŒ‡æ ‡\n    print(f\"\\nğŸ“Š Detailed Classification Report ({prompt_type.upper()}):\")\n    print(classification_report(true_labels, pred_labels, zero_division=0))\n\n    result_filename = f\"prediction_{task_key}_{prompt_type}.csv\"\n    pd.DataFrame(results_records).to_csv(\n        result_filename,\n        index=False,\n        encoding='utf-8-sig'\n    )\n    print(f\"ğŸ’¾ Saved to: {result_filename}\")\n\n    acc = accuracy_score(true_labels, pred_labels)\n    f1 = f1_score(true_labels, pred_labels, average='macro')\n    \n    return {\"accuracy\": acc, \"f1\": f1}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:53.474117Z","iopub.execute_input":"2025-12-27T08:53:53.474515Z","iopub.status.idle":"2025-12-27T08:53:53.496670Z","shell.execute_reply.started":"2025-12-27T08:53:53.474485Z","shell.execute_reply":"2025-12-27T08:53:53.495756Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# é€‰æ‹©ä¸€ä¸ªä»»åŠ¡è¿›è¡Œå¯¹æ¯”ï¼Œä¾‹å¦‚ Kavanaugh æ¨ç‰¹ç«‹åœºåˆ†æ\nTASK_NAME = \"eu_position\" \nSAMPLE_N = None  # å»ºè®®å…ˆç”¨ 20-50 ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•ï¼Œç¡®è®¤æ— è¯¯åå†è·‘å…¨é‡\n\nprint(\"--- 1. Running Baseline (Original Paper Prompts) ---\")\nres_orig = run_comparison(TASK_NAME, prompt_type=\"original\", sample_size=SAMPLE_N)\n\nprint(\"\\n--- 2. Running Experiment (Optimized Prompts) ---\")\nres_opt = run_comparison(TASK_NAME, prompt_type=\"optimized\", sample_size=SAMPLE_N)\n\n# è¾“å‡ºå¯¹æ¯”\nprint(f\"\\nğŸ† === Comparison Result: {TASK_NAME} ===\")\nprint(f\"Original  -> Acc: {res_orig['accuracy']:.4f}, F1: {res_orig['f1']:.4f}\")\nprint(f\"Optimized -> Acc: {res_opt['accuracy']:.4f}, F1: {res_opt['f1']:.4f}\")\n\ndiff_acc = res_opt['accuracy'] - res_orig['accuracy']\nprint(f\"Improvement: {diff_acc:+.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:53:53.497923Z","iopub.execute_input":"2025-12-27T08:53:53.498457Z","iopub.status.idle":"2025-12-27T18:57:27.912010Z","shell.execute_reply.started":"2025-12-27T08:53:53.498377Z","shell.execute_reply":"2025-12-27T18:57:27.910869Z"}},"outputs":[{"name":"stdout","text":"--- 1. Running Baseline (Original Paper Prompts) ---\nğŸ“‚ Detected split files for eu_position. Merging...\nğŸš€ Running eu_position | Mode: ORIGINAL | Samples: 3349 | Start from index: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing:   0%|                                                                                             â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e613fad1d72747e7b653aeedf1cc1a6b"}},"metadata":{}},{"name":"stdout","text":"\nğŸ“Š Detailed Classification Report (ORIGINAL):\n                               precision    recall  f1-score   support\n\nNeutral towards Leave demands       0.12      0.86      0.22       292\n            Pro-Leave demands       0.84      0.40      0.54      2764\n       Very Pro-Leave demands       0.00      0.00      0.00       293\n\n                     accuracy                           0.41      3349\n                    macro avg       0.32      0.42      0.25      3349\n                 weighted avg       0.70      0.41      0.47      3349\n\nğŸ’¾ Saved to: prediction_eu_position_original.csv\n\n--- 2. Running Experiment (Optimized Prompts) ---\nğŸ“‚ Detected split files for eu_position. Merging...\nğŸš€ Running eu_position | Mode: OPTIMIZED | Samples: 3349 | Start from index: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing:   0%|                                                                                             â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4c6ba8bd4e4c03808489664c9d8883"}},"metadata":{}},{"name":"stdout","text":"\nğŸ“Š Detailed Classification Report (OPTIMIZED):\n                               precision    recall  f1-score   support\n\nNeutral towards Leave demands       0.13      0.78      0.23       292\n            Pro-Leave demands       0.85      0.49      0.62      2764\n       Very Pro-Leave demands       0.00      0.00      0.00       293\n\n                     accuracy                           0.48      3349\n                    macro avg       0.33      0.43      0.28      3349\n                 weighted avg       0.71      0.48      0.53      3349\n\nğŸ’¾ Saved to: prediction_eu_position_optimized.csv\n\nğŸ† === Comparison Result: eu_position ===\nOriginal  -> Acc: 0.4055, F1: 0.2530\nOptimized -> Acc: 0.4763, F1: 0.2831\nImprovement: +0.0708\n","output_type":"stream"}],"execution_count":7}]}